{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#4375c7\">Financial Econometrics</span>\n",
    "***\n",
    "*The course material is for educational purposes only. Nothing herein should be taken as an investment advice or  offers any opinion  <br/> with respect to the suitability of any security. To obtain further information about this course, [contact us](http://wp.firrm.de/) or visit us during [office hours](https://finance.wiwi.tu-dortmund.de/professur/team/).*\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands on: Time Series Analysis II\n",
    "\n",
    "In this part the knowledge from the last exercise will be extended and applied to real data. In addition, models assuming heteroscedasticity  model will be applied to calculate value at risk. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core idea VAR\n",
    "- Instead of just analyzing one variable using its past values (like classic AR models), the VAR model looks at two or more variables together.\n",
    "- Each variable in the system is predicted using previous values of all variables, not just its own\n",
    "\n",
    "**What does this mean:**\n",
    "Suppose you want to model two economic indicators, like GDP and unemployment:\n",
    "- With VAR, the current GDP depends on past values of GDP and unemployment.\n",
    "- Likewise, the current unemployment rate depends on past values of unemployment and GDP.​\n",
    "\n",
    "**Basic structure**\n",
    "If you have two variables (let's call them $y_1$ and $y_2$), a simple VAR(1) model (with one lag) looks like this: \n",
    "\n",
    "$$\n",
    "y_{1,t} = a_1 + b_{11} y_{1,t} + b_{12}y_{2,t} + e_{1,t}\n",
    "y_{2,t} = a_1 + b_{21} y_{1,t} + b_{22}y_{2,t} + e_{2,t}\n",
    "$$\n",
    "\n",
    "- $y_{1,t}$ and $y_{2,t}$ are the current values\n",
    "- The coefficients ($b_{ij}$) show how much each lagged variable conributes\n",
    "- $e_{1,t}$ and $e_{2,t}$ are random errors or \"shocks\" which should not be predictable\n",
    "\n",
    "**Difference to autoregressive models with exogenoues variables:**\n",
    "\n",
    "*VAR Model (Vector Autoregression)*\n",
    "- All variables are endogenous, meaning each variable in the system is modeled as depending on past values of itself and the other variables in the system.\n",
    "- Example: If GDP and Inflation are both in the model, both are allowed to be influenced by their own past and each other's past.\n",
    "\n",
    "*ARX or VARX Model (VAR with Exogenous Variables)*\n",
    "- Includes exogenous variables (external, independent variables) that can affect the main variables, but are not themselves predicted by the system.\n",
    "- Exogenous variables are added to the VAR framework to explain (potentially) external influences—these do not receive feedback from the endogenous system.\n",
    "\n",
    "*Quick Analogy*\n",
    "\n",
    "- VAR: All players influence each other, and everyone updates based on everyone else’s past actions.\n",
    "- VARX: Like above, but now you add outside weather reports that influence the players, though the players' actions don’t influence the weather. Weather is exogenous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: VAR Model\n",
    "\n",
    "Use the data files from the previous lecture (`sp500.csv` and `stock_indices.csv`)\n",
    "\n",
    "1. Import the S&P 500 stock market index and the Goldman Sachs stock price. Select prices between 2018-01-01 and 2018-12-31. Visualize both time series.\n",
    "\n",
    "2. Test wether both time series are stationary by using the Augmented Dickey Fuller (ADF) test. If the time series of price are not stationary, calculate the first order difference, i.e. log returns and check for stationarity again.\n",
    "\n",
    "Another statistical property of multiple time series is <b>Cointegration</b>. In generic terms, this means that there is a long-term relationship between the variables [2, p. 254]. \n",
    "\n",
    "3. Apply the [Augmented Engle Granger](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.coint.html) test to check if both time series are cointegrated. Interpret the result.\n",
    "\n",
    "4. Create a VAR model and fit it using both time series. Use `maxlags = 5`. Print the fitted model's summary. \n",
    "\n",
    "5. Forecast both time series 15 days into the future using the fitted VAR model. Visualize the forecast (use `.plot_forecast(...)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller, kpss, coint\n",
    "from statsmodels.tsa.api import VAR\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from arch import arch_model\n",
    "from scipy.stats import norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sp = pd.read_csv('TUdortmund/FinEcon/data/sp500.csv', index_col=0, parse_dates=True)\n",
    "df_stock = pd.read_csv('TUdortmund/FinEcon/data/stock_indices.csv', index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sp = df_sp[(df_sp.index >= '2018-01-01') & (df_sp.index <= '2018-12-31')]\n",
    "df_stock = df_stock[(df_stock.index >= '2018-01-01') & (df_stock.index <= '2018-12-31')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_sp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select GS (Goldman Sachs) from df_sp and SPX (S&P 500) from df_stock for 2018\n",
    "series = pd.DataFrame({\n",
    "    'GS': df_sp['GS'],\n",
    "    'SPX': df_stock['SPX']\n",
    "}).dropna()\n",
    "\n",
    "ax = series.plot(figsize=(10, 5), title='S&P 500 (SPX) vs Goldman Sachs (GS) — 2018')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Price (USD)')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADF test on price series (levels)\n",
    "results_GS = adfuller(series['GS'])\n",
    "results_SPX = adfuller(series['SPX'])\n",
    "\n",
    "def print_adf(res, name):\n",
    "    print(f\"ADF test for {name}:\")\n",
    "    print(f\"  Test statistic: {res[0]:.4f}\")\n",
    "    print(f\"  p-value: {res[1]:.4f}\")\n",
    "    print(f\"  # lags used: {res[2]}\")\n",
    "    print(f\"  # obs used: {res[3]}\")\n",
    "    print(\"  Critical values:\")\n",
    "    for k,v in res[4].items():\n",
    "        print(f\"    {k}: {v:.4f}\")\n",
    "    print()\n",
    "\n",
    "print_adf(results_GS, 'GS (price level)')\n",
    "print_adf(results_SPX, 'SPX (price level)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p-value > 0.05 we fail to reject the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute log returns and re-run ADF on returns\n",
    "rets = np.log(series).diff().dropna()\n",
    "\n",
    "# ADF tests on log returns\n",
    "res_GS_r = adfuller(rets['GS'])\n",
    "res_SPX_r = adfuller(rets['SPX'])\n",
    "print_adf(res_GS_r, 'GS (log returns)')\n",
    "print_adf(res_SPX_r, 'SPX (log returns)')\n",
    "\n",
    "# Quick visualization of returns\n",
    "ax = rets.plot(figsize=(10,4), title='Log returns: GS and SPX (2018)')\n",
    "ax.set_ylabel('Log return')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coint_res = coint(series['GS'], series['SPX'])\n",
    "print('Engle-Granger cointegration test (GS vs SPX):')\n",
    "print(f'  Test statistic: {coint_res[0]:.4f}')\n",
    "print(f'  p-value: {coint_res[1]:.4f}')\n",
    "print('  Critical values (1%, 5%, 10%):')\n",
    "for lvl in coint_res[2]:\n",
    "    print(f'    {lvl:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation: $p-value \\geq 0.05$ -> cannot reject $H_0$ of no cointegration. No evidence of cointegration at conventional levels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpreting the Engle–Granger output**:\n",
    "- The null hypothesis of the test is that the two series are not cointegrated. A small p-value (e.g. < 0.05) means we reject that null and conclude there is evidence of a long-run equilibrium relationship.\n",
    "- If the p-value is large, we do not find evidence for cointegration; the series may wander independently in the long run.\n",
    "- If you find cointegration, subsequent analysis commonly proceeds with an error-correction model (VECM) rather than a standard VAR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = VAR(rets).select_order(5)\n",
    "print('Lag order selection (lower is better):')\n",
    "print(order)\n",
    "\n",
    "model = VAR(rets)\n",
    "res_var = model.fit(maxlags=5)\n",
    "\n",
    "print(res_var.summary())\n",
    "\n",
    "print(f'Number of lags used in fitted VAR: {res_var.k_ar}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 15\n",
    "lag_order = res_var.k_ar\n",
    "last_obs = rets.values[-lag_order:]\n",
    "fc = res_var.forecast(y=last_obs, steps=steps)\n",
    "\n",
    "try:\n",
    "    fc_index = pd.bdate_range(rets.index[-1], periods=steps+1)[1:]\n",
    "except Exception:\n",
    "    fc_index = pd.date_range(rets.index[-1], periods=steps+1, freq='D')[1:]\n",
    "\n",
    "fc_df = pd.DataFrame(fc, index=fc_index, columns=rets.columns)\n",
    "print('Forecast (log returns) — next {} periods:'.format(steps))\n",
    "print(fc_df)\n",
    "\n",
    "plot_df = pd.concat([rets, fc_df])\n",
    "ax = plot_df.plot(figsize=(12,6), title=f'VAR forecast: historical returns and {steps}-step forecast')\n",
    "ax.axvline(rets.index[-1], color='k', linestyle='--', label='Forecast start')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Log return')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "try:\n",
    "    res_var.plot_forecast(steps, figsize=(10,6))\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print('res_var.plot_forecast not available or failed:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARCH and GARCH Models\n",
    "\n",
    "1. What are ARCH and GARCH Models?\n",
    "ARCH (Autoregressive Conditional Heteroskedasticity) and GARCH (Generalized ARCH) are models that allow the variance (volatility) of a time series to change over time—unlike standard models that assume constant volatility.\n",
    "\n",
    "---\n",
    "\n",
    "2. Why Use Them?\n",
    "- Many financial series show **volatility clustering**: big changes tend to be followed by other big changes.\n",
    "- ARCH/GARCH models help capture and forecast this dynamic behavior.\n",
    "\n",
    "---\n",
    "\n",
    "3. ARCH Model\n",
    "ARCH(q): The variance at time \\( t \\) depends on the squared errors from the previous \\( q \\) periods.\n",
    "\n",
    "$$\n",
    "\\epsilon_t = \\sigma_t w_t \\\\\n",
    "\\sigma_t^2 = \\alpha_0 + \\alpha_1 \\epsilon_{t-1}^2 + \\cdots + \\alpha_q \\epsilon_{t-q}^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "4. GARCH Model\n",
    "GARCH(p, q): The variance depends on both earlier squared errors (like ARCH) **and** earlier variances.\n",
    "$$\n",
    "\\sigma_t^2 = \\alpha_0 + \\sum_{i=1}^q \\alpha_i \\epsilon_{t-i}^2 + \\sum_{j=1}^p \\beta_j \\sigma_{t-j}^2\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: GARCH Model and Value at Risk\n",
    "\n",
    "Install the `arch` package if necessary.\n",
    "\n",
    "1. Visualize  the returns of Goldman Sachs in a plot as well as in a histogram.\n",
    "\n",
    "2. Create a garch model using `arch_model` using the returns of Goldman Sachs. \n",
    "\n",
    "Hint: The estimation of the model parameter is done by maximum likelihood estimation. Small numbers can negatively influence the minimization process. Rescale your returns by a scalar, e.g. `100`.\n",
    "\n",
    "3. Fit the model and print the summary.\n",
    "\n",
    "4. Store the conditional volatility in an array `cond_vola`.\n",
    "\n",
    "5. Calculate the value at risk for every $t$ using the conditional volatility for $\\alpha = 0.05$ :\n",
    "$$\n",
    "\\text{VaR}(t, \\alpha) = \\mu + \\sigma({t|t-1})\\,\\Phi^{-1}(\\alpha)\n",
    "$$\n",
    "\n",
    "$\\Phi^{-1}$ is the inverse cumulative distribution function of a normal distribution. Use `scipy.stats.norm.ppf(alpha)`. $\\mu$ is the mean value of the returns.\n",
    "\n",
    "6. Visualize both the Value at Risk and the returns in a plot.\n",
    "\n",
    "7. Calculate the VaR($\\alpha$) based on historical returns, i.e. calculate the $\\alpha$ quantile of the returns. Implement the result as a horizontal line in the plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gs_rets = rets['GS'].dropna()\n",
    "print('Observations:', len(gs_rets))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "gs_rets.plot(ax=axes[0], title='GS Log Returns (2018)', color='C0')\n",
    "axes[0].set_ylabel('Log return')\n",
    "axes[0].axhline(0, color='k', linewidth=0.6)\n",
    "\n",
    "axes[1].hist(gs_rets, bins=40, density=True, alpha=0.6, color='C1')\n",
    "gs_rets.plot(kind='density', ax=axes[1], color='C2')\n",
    "axes[1].set_title('Histogram and density: GS log returns')\n",
    "axes[1].set_xlabel('Log return')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am = arch_model(gs_rets * 100, vol='Garch', p=1, q=1, mean='Constant', dist='normal')\n",
    "res = am.fit(disp='off')\n",
    "print(res.summary())\n",
    "\n",
    "cond_vola = res.conditional_volatility / 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "gs_rets = gs_rets.dropna()\n",
    "cond_vola = pd.Series(cond_vola, index=gs_rets.index)\n",
    "\n",
    "z = norm.ppf(alpha)\n",
    "mu = gs_rets.mean()\n",
    "var_series = mu + cond_vola * z\n",
    "\n",
    "rolling_hist_var = gs_rets.rolling(window=250, min_periods=1).quantile(alpha)\n",
    "historical_var = gs_rets.quantile(alpha)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "gs_rets.plot(ax=ax, label='GS returns', color='C0', alpha=0.6)\n",
    "var_series.plot(ax=ax, label=f'GARCH VaR (alpha={alpha})', color='C1')\n",
    "rolling_hist_var.plot(ax=ax, label=f'Rolling hist. VaR (window=250, alpha={alpha})', color='C3')\n",
    "ax.axhline(historical_var, color='k', linestyle='--', label=f'Historical VaR (alpha={alpha})')\n",
    "ax.set_ylabel('Log return')\n",
    "ax.set_title('GS returns and Value at Risk')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "***\n",
    "[1] Park, K. I., & Park. (2018). Fundamentals of Probability and Stochastic Processes with Applications to Communications.\n",
    "\n",
    "[2] Brockwell, P. J., & Davis, R. A. (2016). Introduction to time series and forecasting.\n",
    "\n",
    "[3] Montgomery, D. C., Jennings, C. L., & Kulahci, M. (2015). Introduction to time series analysis and forecasting. \n",
    "\n",
    "[4] Box, G. E., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). Time series analysis: forecasting and control. \n",
    "\n",
    "[5] Kelepouris, I., Kelepouris, D. Value at Risk estimation using GARCH model. [URL](https://www.kaggle.com/ionaskel/value-at-risk-estimation-using-garch-model) (visited: 2020-12-09)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
